{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from feedback import *\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Sequence data shape: (20592, 29247)\n",
      "Total Sequence diff lenght: 20592\n"
     ]
    }
   ],
   "source": [
    "# sequence numpy array data\n",
    "seq_npy_data_path = './np_image_totalunit/BA-20000-tsne-binary-perplexity=50-pixel=100/total_seq_array.npy'\n",
    "load_data = np.load(seq_npy_data_path)\n",
    "print(\"Total Sequence data shape: {}\".format(load_data.shape))\n",
    "# Sequence diff label \n",
    "seq_diff_data_path = './np_image_totalunit/BA-20000-tsne-binary-perplexity=50-pixel=100/label.npy'\n",
    "load_diff_lab = np.load(seq_diff_data_path, allow_pickle=True)\n",
    "print(\"Total Sequence diff lenght: {}\".format(len(load_diff_lab)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyDeepInsight.image_transformer import ImageTransformer, LogScaler\n",
    "from tsnecuda import TSNE\n",
    "from tqdm.notebook import tqdm\n",
    "ln = LogScaler()\n",
    "load_data_norm  = ln.fit_transform(load_data)\n",
    "# binary process\n",
    "load_diff_lab_norm = [0 if diff=='N' else 1 for  diff in load_diff_lab]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15444 15444\n",
      "5148 5148\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "sss = StratifiedShuffleSplit(n_splits=2, test_size=0.25, random_state=42)\n",
    "\n",
    "train_indx, test_indx = next(sss.split(load_data_norm, load_diff_lab_norm))\n",
    "train_ids = [load_data_norm[ind] for ind in train_indx]\n",
    "train_labels = [load_diff_lab_norm[ind] for ind in train_indx]\n",
    "print(len(train_ids), len(train_labels)) \n",
    "\n",
    "test_ids = [load_data_norm[ind] for ind in test_indx]\n",
    "test_labels = [load_diff_lab_norm[ind] for ind in test_indx]\n",
    "print(len(test_ids), len(test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "import torchvision.transforms as transforms\n",
    "import glob\n",
    "from PIL import Image\n",
    "import torch\n",
    "import numpy as np\n",
    "import random\n",
    "np.random.seed(2020)\n",
    "random.seed(2020)\n",
    "torch.manual_seed(2020)\n",
    "\n",
    "class TransferDataset(Dataset):\n",
    "    def __init__(self, ids, labels, transform):\n",
    "        self.transform = transform\n",
    "        self.ids = ids\n",
    "        self.labels = labels\n",
    "    def __len__(self):\n",
    "        return len(self.ids)\n",
    "    def __getitem__(self, idx):\n",
    "        singel_image_ = self.ids[idx].astype(np.float32)\n",
    "        singel_image_ = singel_image_.flatten()\n",
    "        seed = np.random.randint(1e9)       \n",
    "        random.seed(seed)\n",
    "        np.random.seed(seed)\n",
    "        singel_image_ = torch.unsqueeze(torch.FloatTensor(singel_image_), 0)\n",
    "        # singel_image_ = torch.unsqueeze(self.transform(singel_image_)[0], axis=0)\n",
    "        label = int(self.labels[idx])\n",
    "\n",
    "        return singel_image_, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15444 5148\n"
     ]
    }
   ],
   "source": [
    "transformer = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            # transforms.Normalize(mean, std),\n",
    "            ])     \n",
    "\n",
    "train_ds = TransferDataset(ids= train_ids, labels= train_labels, transform= transformer)\n",
    "test_ds = TransferDataset(ids= test_ids, labels= test_labels, transform= transformer)\n",
    "print(len(train_ds), len(test_ds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "train_dl = DataLoader(train_ds, batch_size= batch_size, \n",
    "                        shuffle=True)\n",
    "test_dl = DataLoader(test_ds, batch_size= 2*batch_size, \n",
    "                        shuffle=False)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.7325, 0.0000, 0.8856,  ..., 0.0000, 0.7325, 0.8856]])\n",
      "tensor(1)\n",
      "torch.Size([64, 1, 29247]) torch.Size([64])\n"
     ]
    }
   ],
   "source": [
    "for xb,yb in train_dl:\n",
    "    # print(xb.shape, yb.shape)\n",
    "    # print(xb[0][0][0][0])\n",
    "    print(xb[0])\n",
    "    print(yb[0])\n",
    "    break\n",
    "\n",
    "for xb,yb in test_dl:\n",
    "    print(xb.shape, yb.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.7.1'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, class_n, Input_Size):\n",
    "        super(RNN, self).__init__()\n",
    "\n",
    "        self.rnn = nn.LSTM(\n",
    "            input_size=Input_Size,\n",
    "            hidden_size=128,\n",
    "            num_layers=1,\n",
    "            batch_first=True,\n",
    "        )\n",
    "\n",
    "        self.out = nn.Linear(128, class_n)\n",
    "    def forward(self, x):\n",
    "        r_out, (h_c, h_h) = self.rnn(x, None)\n",
    "        out = self.out(r_out[:, -1, :])\n",
    "        return out\n",
    "    \n",
    "    def initialize_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                torch.nn.init.normal_(m.weight.data, 0, 0.01)\n",
    "                # m.weight.data.normal_(0,0.01)\n",
    "                m.bias.data.zero_()\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "model = RNN(2, 29247).to(device)\n",
    "model.initialize_weights()\n",
    "# model.features[0] = nn.Conv2d(1, 64, kernel_size=(11, 11), stride=(4, 4), padding=(2, 2))\n",
    "# model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import optim\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR, ReduceLROnPlateau\n",
    "\n",
    "# loss_func = nn.CrossEntropyLoss(reduction=\"sum\", weight=class_weights)\n",
    "loss_func = nn.CrossEntropyLoss(reduction=\"sum\")\n",
    "opt = optim.Adam(model.parameters(), lr=0.003)\n",
    "lr_scheduler = ReduceLROnPlateau(opt, mode='min',factor=0.5, patience=5,verbose=1)\n",
    "os.makedirs(\"./models\", exist_ok=True)\n",
    "path2weights = \"./models/BA_20000_weights_LSTM_diffclass.pt\"\n",
    "params_train={\n",
    "    \"num_epochs\": 10,\n",
    "    \"optimizer\": opt,\n",
    "    \"loss_func\": loss_func,\n",
    "    \"train_dl\": train_dl,\n",
    "    \"val_dl\": test_dl,\n",
    "    \"sanity_check\": False,\n",
    "    \"lr_scheduler\": lr_scheduler,\n",
    "    \"path2weights\": path2weights,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "from tqdm import tqdm_notebook\n",
    "def loss_epoch(model,loss_func,dataset_dl,sanity_check=False,opt=None):\n",
    "    running_loss=0.0\n",
    "    running_metric=0.0\n",
    "    len_data = len(dataset_dl.dataset)\n",
    "    for xb, yb in tqdm_notebook(dataset_dl):\n",
    "    # for xb, yb in (dataset_dl):\n",
    "        xb=xb.to(device)\n",
    "        yb=yb.to(device)\n",
    "        # print(type(xb), type(yb.shape))\n",
    "        output=model(xb)\n",
    "        loss_b,metric_b=loss_batch(loss_func, output, yb, opt)\n",
    "        running_loss+=loss_b\n",
    "        \n",
    "        if metric_b is not None:\n",
    "            running_metric+=metric_b\n",
    "        if sanity_check is True:\n",
    "            break\n",
    "    loss=running_loss/float(len_data)\n",
    "    metric=running_metric/float(len_data)\n",
    "    return loss, metric\n",
    "\n",
    "def get_lr(opt):\n",
    "    for param_group in opt.param_groups:\n",
    "        return param_group['lr']\n",
    "\n",
    "def metrics_batch(output, target):\n",
    "    pred = output.argmax(dim=1, keepdim=True)\n",
    "    corrects=pred.eq(target.view_as(pred)).sum().item()\n",
    "    return corrects\n",
    "\n",
    "def loss_batch(loss_func, output, target, opt=None):\n",
    "    loss = loss_func(output, target)\n",
    "    with torch.no_grad():\n",
    "        metric_b = metrics_batch(output,target)\n",
    "    if opt is not None:\n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "    return loss.item(), metric_b\n",
    "\n",
    "def train_val(model, params):\n",
    "    num_epochs=params[\"num_epochs\"]\n",
    "    loss_func=params[\"loss_func\"]\n",
    "    opt=params[\"optimizer\"]\n",
    "    train_dl=params[\"train_dl\"]\n",
    "    val_dl=params[\"val_dl\"]\n",
    "    sanity_check=params[\"sanity_check\"]\n",
    "    lr_scheduler=params[\"lr_scheduler\"]\n",
    "    path2weights=params[\"path2weights\"]\n",
    "    \n",
    "    loss_history={\n",
    "        \"train\": [],\n",
    "        \"val\": [],\n",
    "    }\n",
    "    \n",
    "    metric_history={\n",
    "        \"train\": [],\n",
    "        \"val\": [],\n",
    "    }\n",
    "    \n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_loss=float('inf')\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        current_lr=get_lr(opt)\n",
    "        print('Epoch {}/{}, current lr={}'.format(epoch, num_epochs - 1, current_lr))\n",
    "        model.train()\n",
    "        train_loss, train_metric=loss_epoch(model,loss_func,train_dl,sanity_check,opt)\n",
    "        loss_history[\"train\"].append(train_loss)\n",
    "        metric_history[\"train\"].append(train_metric)\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_loss, val_metric=loss_epoch(model,loss_func,val_dl,sanity_check)\n",
    "        if val_loss < best_loss:\n",
    "            best_loss = val_loss\n",
    "            best_model_wts = copy.deepcopy(model.state_dict())\n",
    "            torch.save(model.state_dict(), path2weights)\n",
    "            print(\"Copied best model weights!\")\n",
    "        \n",
    "        loss_history[\"val\"].append(val_loss)\n",
    "        metric_history[\"val\"].append(val_metric)\n",
    "        \n",
    "        lr_scheduler.step(val_loss)\n",
    "        if current_lr != get_lr(opt):\n",
    "            print(\"Loading best model weights!\")\n",
    "            model.load_state_dict(best_model_wts)\n",
    "        \n",
    "\n",
    "        print(\"train loss: %.6f, dev loss: %.6f,  train accuracy: %.2f,valid accuracy: %.2f\" %(train_loss,val_loss, 100*train_metric,100*val_metric))\n",
    "        print(\"-\"*10) \n",
    "    model.load_state_dict(best_model_wts)\n",
    "        \n",
    "    return model, loss_history, metric_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/9, current lr=0.003\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/john/anaconda3/envs/deepinsight/lib/python3.7/site-packages/ipykernel_launcher.py:7: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  import sys\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "31ed351e08db4f4eb37dc87b6f4f4b8d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/483 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "972de72ee74b43598cb74054f3671d96",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/81 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copied best model weights!\n",
      "train loss: 0.312054, dev loss: 0.309258,  train accuracy: 90.63,valid accuracy: 90.79\n",
      "----------\n",
      "Epoch 1/9, current lr=0.003\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "57ee133e6b61408c8aa757ad94fa844b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/483 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e8be931f7f8941e4a381cf6010c22e4b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/81 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss: 0.309906, dev loss: 0.317756,  train accuracy: 90.79,valid accuracy: 90.79\n",
      "----------\n",
      "Epoch 2/9, current lr=0.003\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "30c87d3859c54471aafe3e2c56316edb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/483 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "594a1e756a9840cfb6ce0bbefb9a63b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/81 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss: 0.310993, dev loss: 0.311800,  train accuracy: 90.79,valid accuracy: 90.79\n",
      "----------\n",
      "Epoch 3/9, current lr=0.003\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b2b25048962f44ceb0f0f4e60b6b5676",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/483 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "17dd7dcbdae947fc8d7fa24f63fc6235",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/81 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copied best model weights!\n",
      "train loss: 0.310764, dev loss: 0.308421,  train accuracy: 90.79,valid accuracy: 90.79\n",
      "----------\n",
      "Epoch 4/9, current lr=0.003\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d40872e5c24f48ff81e27d4ecf1b07d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/483 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75cb8c5517f344a482720281a6689ee7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/81 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss: 0.310716, dev loss: 0.309486,  train accuracy: 90.79,valid accuracy: 90.79\n",
      "----------\n",
      "Epoch 5/9, current lr=0.003\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "04b3f3d949e242698ec6d20dae5cf93d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/483 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d34969eec1949cea14b3bc4efd11d5d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/81 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss: 0.310508, dev loss: 0.312921,  train accuracy: 90.79,valid accuracy: 90.79\n",
      "----------\n",
      "Epoch 6/9, current lr=0.003\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "045f534763ba496ea00289e394873afb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/483 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f48ace858d64a5fb3f97730268d9214",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/81 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copied best model weights!\n",
      "train loss: 0.311378, dev loss: 0.307561,  train accuracy: 90.79,valid accuracy: 90.79\n",
      "----------\n",
      "Epoch 7/9, current lr=0.003\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e161ec3230249489e5a3b3dcc0b9e2f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/483 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "722d6f20b2344319bfe2368c055fc6df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/81 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss: 0.310446, dev loss: 0.313072,  train accuracy: 90.79,valid accuracy: 90.79\n",
      "----------\n",
      "Epoch 8/9, current lr=0.003\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "07e8d24ebfd94f0ab7b9f48d6e0135f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/483 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "23ab566efca341c087c37f51da08980d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/81 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss: 0.310577, dev loss: 0.308672,  train accuracy: 90.79,valid accuracy: 90.79\n",
      "----------\n",
      "Epoch 9/9, current lr=0.003\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "384d2a2f7da04b5290ef352e0b11eaa9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/483 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a1e1db765614d43abb2ac75e5270189",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/81 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copied best model weights!\n",
      "train loss: 0.310061, dev loss: 0.307377,  train accuracy: 90.79,valid accuracy: 90.79\n",
      "----------\n"
     ]
    }
   ],
   "source": [
    "model,loss_hist,metric_hist = train_val(model,params_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.11 ('deepinsight')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c7e03d6feef9f460a8719d2376e3019088330392fc43676d10494b1071d44477"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
