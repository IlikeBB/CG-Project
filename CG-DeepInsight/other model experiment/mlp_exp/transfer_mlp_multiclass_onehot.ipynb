{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from backfeed import *\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nas_path = \"./dataset_1401/\"\n",
    "lineage_label = pd.read_csv('./dataset_1401/1404_lineage_report and metadata 20220316.csv')[['scorpio_call_y','diff']]\n",
    "lineage_label = np.array(lineage_label.fillna(\"None\"))\n",
    "label_ = []\n",
    "new_lineage_label = []\n",
    "for idx, rna in enumerate(SeqIO.parse('./dataset_1401/1404.sequences.aln.fasta',\"fasta\")):\n",
    "    # print(lineage_label[idx][0].split(' ')[0])\n",
    "    label_.append(lineage_label[idx][0].split(' ')[0])\n",
    "\n",
    "    new_lineage_label.append(str(rna.seq).replace('-','N'))\n",
    "\n",
    "print('sample:', len(new_lineage_label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(x):\n",
    "\tx = x.upper() \n",
    "\t\n",
    "\tif x == 'T' or x == 'A' or x == 'G' or x == 'C' or x == '-' or x == 'N':\n",
    "\t\treturn x\n",
    "\n",
    "\tif x == 'U' or x == 'Y':\n",
    "\t\treturn 'T'\n",
    "\t\n",
    "\tif x == 'K' or x == 'S':\n",
    "\t\treturn 'G'\n",
    "\n",
    "\tif x == 'M' or x == 'R' or x == 'W' or x == 'H' or x=='V' or x=='D':\n",
    "\t\treturn 'A'\n",
    "\n",
    "\tif x== 'B':\n",
    "\t\treturn 'C'\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "dict_search = {'N':[1,0,0,0,0], 'A':[0,1,0,0,0], 'C':[0,0,1,0,0], 'G':[0,0,0,1,0], 'T':[0,0,0,0,1]}\n",
    "total_sequence_array_onehot = []\n",
    "for Seq in tqdm(new_lineage_label):\n",
    "    temp_one_hot_Seq = []\n",
    "    for nactg in Seq:\n",
    "        unit = clean(nactg)\n",
    "        temp_one_hot_Seq.append(dict_search[unit])\n",
    "    total_sequence_array_onehot.append(np.array(temp_one_hot_Seq))\n",
    "total_sequence_array_onehot = np.array(total_sequence_array_onehot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_sequence_array_onehot.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_sequence_array_onehot[0][500:505], new_lineage_label[0][500:505]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyDeepInsight import ImageTransformer, LogScaler\n",
    "from tsnecuda import TSNE\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_,_ ,_,_= np.unique(label_,return_counts=True,return_index=True,return_inverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_dict_ = {}\n",
    "for idx, i in enumerate(class_):\n",
    "    class_dict_[i] = idx\n",
    "print(class_dict_)\n",
    "multi_label = []\n",
    "for i in label_:\n",
    "    multi_label.append(class_dict_[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "sss = StratifiedShuffleSplit(n_splits=2, test_size=0.25, random_state=42)\n",
    "\n",
    "train_indx, test_indx = next(sss.split(total_sequence_array_onehot, multi_label))\n",
    "train_ids = [total_sequence_array_onehot[ind] for ind in train_indx]\n",
    "train_labels = [multi_label[ind] for ind in train_indx]\n",
    "print(len(train_ids), len(train_labels)) \n",
    "\n",
    "test_ids = [total_sequence_array_onehot[ind] for ind in test_indx]\n",
    "test_labels = [multi_label[ind] for ind in test_indx]\n",
    "print(len(test_ids), len(test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "import torchvision.transforms as transforms\n",
    "import glob\n",
    "from PIL import Image\n",
    "import torch\n",
    "import numpy as np\n",
    "import random\n",
    "np.random.seed(2020)\n",
    "random.seed(2020)\n",
    "torch.manual_seed(2020)\n",
    "\n",
    "class TransferDataset(Dataset):\n",
    "    def __init__(self, ids, labels, transform):\n",
    "        self.transform = transform\n",
    "        self.ids = ids\n",
    "        self.labels = labels\n",
    "    def __len__(self):\n",
    "        return len(self.ids)\n",
    "    def __getitem__(self, idx):\n",
    "        singel_image_ = self.ids[idx].astype(np.float32)\n",
    "        singel_image_ = singel_image_.flatten()\n",
    "        seed = np.random.randint(1e9)       \n",
    "        random.seed(seed)\n",
    "        np.random.seed(seed)\n",
    "        singel_image_ = torch.FloatTensor(singel_image_)\n",
    "        # singel_image_ = torch.unsqueeze(self.transform(singel_image_)[0], axis=0)\n",
    "        label = int(self.labels[idx])\n",
    "\n",
    "        return singel_image_, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            # transforms.Normalize(mean, std),\n",
    "            ])     \n",
    "\n",
    "train_ds = TransferDataset(ids= train_ids, labels= train_labels, transform= transformer)\n",
    "test_ds = TransferDataset(ids= test_ids, labels= test_labels, transform= transformer)\n",
    "print(len(train_ds), len(test_ds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs, label = train_ds[10]\n",
    "batch_size = 32\n",
    "train_dl = DataLoader(train_ds, batch_size= batch_size, \n",
    "                        shuffle=True)\n",
    "test_dl = DataLoader(test_ds, batch_size= 2*batch_size, \n",
    "                        shuffle=False)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for xb,yb in train_dl:\n",
    "    # print(xb.shape, yb.shape)\n",
    "    # print(xb[0][0][0][0])\n",
    "    print(yb[0])\n",
    "    break\n",
    "\n",
    "for xb,yb in test_dl:\n",
    "    print(xb.shape, yb.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, num_classes, input_size):\n",
    "        super(MLP,self).__init__()\n",
    "        self.linear1 = nn.Linear(in_features=input_size, out_features=1024)\n",
    "        self.bn1 = nn.BatchNorm1d(1024)\n",
    "        self.dt1 = nn.Dropout(0.25)\n",
    "        self.linear2 = nn.Linear(in_features=1024, out_features=256)\n",
    "        self.bn2 = nn.BatchNorm1d(256)\n",
    "        self.dt2 = nn.Dropout(0.25)\n",
    "        self.linear3 = nn.Linear(in_features=256, out_features= num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.bn1(self.linear1(x))\n",
    "        x = F.relu(x)\n",
    "        # x = self.dt1(x)\n",
    "        x = self.bn2(self.linear2(x))\n",
    "        x = F.relu(x)\n",
    "        # x = self.dt2(x)\n",
    "        x = self.linear3(x)\n",
    "        x = F.softmax(x, dim=1)\n",
    "        return x\n",
    "        \n",
    "    def initialize_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.BatchNorm1d):\n",
    "                m.weight.data.fill_(1)\n",
    "                m.bias.data.zero_()\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                torch.nn.init.normal_(m.weight.data, 0, 0.01)\n",
    "                # m.weight.data.normal_(0,0.01)\n",
    "                m.bias.data.zero_()\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = MLP(len(class_), 149515).to(device)\n",
    "model.initialize_weights()\n",
    "# model.features[0] = nn.Conv2d(1, 64, kernel_size=(11, 11), stride=(4, 4), padding=(2, 2))\n",
    "# model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import optim\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR, ReduceLROnPlateau\n",
    "\n",
    "# loss_func = nn.CrossEntropyLoss(reduction=\"sum\", weight=class_weights)\n",
    "loss_func = nn.CrossEntropyLoss(reduction=\"sum\")\n",
    "opt = optim.Adam(model.parameters(), lr=0.003)\n",
    "lr_scheduler = ReduceLROnPlateau(opt, mode='min',factor=0.5, patience=5,verbose=1)\n",
    "os.makedirs(\"./models\", exist_ok=True)\n",
    "path2weights = \"./models/weights_MLP_multiclass(onehot).2022.03.22.pt\"\n",
    "params_train={\n",
    "    \"num_epochs\": 100,\n",
    "    \"optimizer\": opt,\n",
    "    \"loss_func\": loss_func,\n",
    "    \"train_dl\": train_dl,\n",
    "    \"val_dl\": test_dl,\n",
    "    \"sanity_check\": False,\n",
    "    \"lr_scheduler\": lr_scheduler,\n",
    "    \"path2weights\": path2weights,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "from tqdm import tqdm_notebook\n",
    "def loss_epoch(model,loss_func,dataset_dl,sanity_check=False,opt=None):\n",
    "    running_loss=0.0\n",
    "    running_metric=0.0\n",
    "    len_data = len(dataset_dl.dataset)\n",
    "    for xb, yb in tqdm_notebook(dataset_dl):\n",
    "    # for xb, yb in (dataset_dl):\n",
    "        xb=xb.to(device)\n",
    "        yb=yb.to(device)\n",
    "        # print(type(xb), type(yb.shape))\n",
    "        output=model(xb)\n",
    "        loss_b,metric_b=loss_batch(loss_func, output, yb, opt)\n",
    "        running_loss+=loss_b\n",
    "        \n",
    "        if metric_b is not None:\n",
    "            running_metric+=metric_b\n",
    "        if sanity_check is True:\n",
    "            break\n",
    "    loss=running_loss/float(len_data)\n",
    "    metric=running_metric/float(len_data)\n",
    "    return loss, metric\n",
    "\n",
    "def get_lr(opt):\n",
    "    for param_group in opt.param_groups:\n",
    "        return param_group['lr']\n",
    "\n",
    "def metrics_batch(output, target):\n",
    "    pred = output.argmax(dim=1, keepdim=True)\n",
    "    corrects=pred.eq(target.view_as(pred)).sum().item()\n",
    "    return corrects\n",
    "\n",
    "def loss_batch(loss_func, output, target, opt=None):\n",
    "    loss = loss_func(output, target)\n",
    "    with torch.no_grad():\n",
    "        metric_b = metrics_batch(output,target)\n",
    "    if opt is not None:\n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "    return loss.item(), metric_b\n",
    "\n",
    "def train_val(model, params):\n",
    "    num_epochs=params[\"num_epochs\"]\n",
    "    loss_func=params[\"loss_func\"]\n",
    "    opt=params[\"optimizer\"]\n",
    "    train_dl=params[\"train_dl\"]\n",
    "    val_dl=params[\"val_dl\"]\n",
    "    sanity_check=params[\"sanity_check\"]\n",
    "    lr_scheduler=params[\"lr_scheduler\"]\n",
    "    path2weights=params[\"path2weights\"]\n",
    "    \n",
    "    loss_history={\n",
    "        \"train\": [],\n",
    "        \"val\": [],\n",
    "    }\n",
    "    \n",
    "    metric_history={\n",
    "        \"train\": [],\n",
    "        \"val\": [],\n",
    "    }\n",
    "    \n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_loss=float('inf')\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        current_lr=get_lr(opt)\n",
    "        print('Epoch {}/{}, current lr={}'.format(epoch, num_epochs - 1, current_lr))\n",
    "        model.train()\n",
    "        train_loss, train_metric=loss_epoch(model,loss_func,train_dl,sanity_check,opt)\n",
    "        loss_history[\"train\"].append(train_loss)\n",
    "        metric_history[\"train\"].append(train_metric)\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_loss, val_metric=loss_epoch(model,loss_func,val_dl,sanity_check)\n",
    "        if val_loss < best_loss:\n",
    "            best_loss = val_loss\n",
    "            best_model_wts = copy.deepcopy(model.state_dict())\n",
    "            torch.save(model.state_dict(), path2weights)\n",
    "            print(\"Copied best model weights!\")\n",
    "        \n",
    "        loss_history[\"val\"].append(val_loss)\n",
    "        metric_history[\"val\"].append(val_metric)\n",
    "        \n",
    "        lr_scheduler.step(val_loss)\n",
    "        if current_lr != get_lr(opt):\n",
    "            print(\"Loading best model weights!\")\n",
    "            model.load_state_dict(best_model_wts)\n",
    "        \n",
    "\n",
    "        print(\"train loss: %.6f, dev loss: %.6f,  train accuracy: %.2f,valid accuracy: %.2f\" %(train_loss,val_loss, 100*train_metric,100*val_metric))\n",
    "        print(\"-\"*10) \n",
    "    model.load_state_dict(best_model_wts)\n",
    "        \n",
    "    return model, loss_history, metric_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model,loss_hist,metric_hist = train_val(model,params_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "cefeaee0cb99e52f47ecbf6a0fec4d636206690d7e9c62031f057a9471691d65"
  },
  "kernelspec": {
   "display_name": "Python 3.6.13 ('lstm_pyt')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
