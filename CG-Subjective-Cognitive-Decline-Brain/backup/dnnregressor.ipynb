{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL']='3'\n",
    "import openpyxl\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import scipy.stats as stats\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将各类变量放在一个位置集中定义，十分有利于机器学习等变量较多的代码\n",
    "MyModelPath=\"./model_save/\" # 确定每一次训练所得模型保存的位置\n",
    "MyDataPath=\"./data/image data/mdALFF matrix excel/mdALFF.csv\" # 确定输入数据的位置\n",
    "MyResultSavePath=f\"./model_save/{datetime.now().strftime('%D %H:%M:%S')} - Result.xlsx\" # 确定模型精度结果（RMSE等）与模型参数保存的位置\n",
    "TestSize=0.2 # 确定数据中测试集所占比例\n",
    "RandomSeed=np.random.randint(low=24,high=25) # 确定划分训练集与测试集的随机数种子\n",
    "OptMethod='Adam' # 确定模型所用的优化方法\n",
    "LearningRate=0.001 # 确定学习率\n",
    "DecayStep=200 # 确定学习率下降的步数\n",
    "DecayRate=0.96 # 确定学习率下降比率\n",
    "HiddenLayer=[50,100] # 确定隐藏层数量与每一层对应的神经元数量\n",
    "ActFun='tf.nn.relu' # 确定激活函数\n",
    "Dropout=0.5 # 确定Dropout的值\n",
    "LossReduction='tf.compat.v1.ReductionV2.SUM_OVER_BATCH_SIZE' # 指定每个批次训练误差的减小方法\n",
    "BatchNorm='False' # 确定是否使用Batch Normalizing\n",
    "TrainBatchSize=20 # 确定训练数据一个Batch的大小\n",
    "TrainStep=3000 # 确定训练数据的Step数量\n",
    "EvalBatchSize=1 # 确定验证数据一个Batch的大小\n",
    "PredictBatchSize=1 # 确定预测数据（即测试集）一个Batch的大小"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LoadData函数，加载全部数据\n",
    "def LoadData(DataPath):\n",
    "    MyData=pd.read_csv(DataPath) # 加载DataPath路径所指定的数据，names中的内容为各列的名称\n",
    "\n",
    "    MyData = MyData[MyData.columns[1::]]\n",
    "    return MyData\n",
    "\n",
    "# 初始数据处理\n",
    "AllX=LoadData(MyDataPath) # 调用LoadData函数，获取数据\n",
    "clinical_data = pd.read_csv('./data/clinical data/clinical_data.csv')[['subj', 'AD8主觀認知障礙', 'MOCA客觀認知測驗分數', 'HADS_A焦慮程度', 'HADS_D憂鬱程度']]\n",
    "patient_value_index = clinical_data['MOCA客觀認知測驗分數']\n",
    "\n",
    "\n",
    "\n",
    "# 划分数据训练集与测试集\n",
    "TrainX,TestX,TrainY,TestY=train_test_split(AllX,\n",
    "                                           patient_value_index,\n",
    "                                           test_size=TestSize, # 指定数据中测试集所占比例\n",
    "                                           random_state=RandomSeed # 指定划分训练集与测试集的随机数种子\n",
    "                                           )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "FeatureColumn=[] # 定义一个新的“Feature columns”对象\n",
    "for key in AllX.keys():\n",
    "    FeatureColumn.append(tf.feature_column.numeric_column(key=key)) # 将全部因变量数据（需要均为连续变量）导入"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizer=OptMethod # 优化方法选用OptMethod所指定的方法\n",
    "Optimizer=lambda:tf.keras.optimizers.Adam(\n",
    "    learning_rate=tf.compat.v1.train.exponential_decay(learning_rate=LearningRate, # 初始学习率\n",
    "                                                       global_step=tf.compat.v1.train.get_global_step(),\n",
    "                                                       # 全局步数，用以计算已经衰减后的学习率\n",
    "                                                       # get_global_step()函数自动获取当前的已经执行的步数\n",
    "                                                       decay_steps=DecayStep, # 学习率下降完成的指定步数\n",
    "                                                       decay_rate=DecayRate # 衰减率\n",
    "                                                       ) # 选用基于学习率指数下降的Adam方法，此举有助于降低过拟合风险\n",
    "                                                         # 这一函数返回每次对应的学习率\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using default config.\n",
      "INFO:tensorflow:Using config: {'_model_dir': './model_save/', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': allow_soft_placement: true\n",
      "graph_options {\n",
      "  rewrite_options {\n",
      "    meta_optimizer_iterations: ONE\n",
      "  }\n",
      "}\n",
      ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_checkpoint_save_graph_def': True, '_service': None, '_cluster_spec': ClusterSpec({}), '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.profiler import trace\n",
    "# 基于DNNRegressor构建深度学习模型\n",
    "DNNModel=tf.estimator.DNNRegressor(feature_columns=FeatureColumn, # 指定模型所用的“Feature columns”对象\n",
    "                                   hidden_units=HiddenLayer, # 指定隐藏层数量与每一层对应的神经元数量\n",
    "                                   optimizer=Optimizer, # 指定模型所用的优化方法                                  \n",
    "                                   activation_fn=eval(ActFun), # 指定激活函数\n",
    "                                   dropout=Dropout, # 指定Dropout的值\n",
    "                                   label_dimension=1, # 输出数据的维度，即因变量的个数\n",
    "                                   model_dir=MyModelPath, # 指定每一次训练所得模型保存的位置\n",
    "                                   # loss_reduction=eval(LossReduction), # 指定每个批次训练误差的减小方法\n",
    "                                   batch_norm=eval(BatchNorm) # 指定是否使用Batch Normalizing\n",
    "                                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/john/anaconda3/envs/SCD/lib/python3.7/site-packages/tensorflow/python/training/training_util.py:236: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Calling checkpoint listeners before saving checkpoint 0...\n",
      "INFO:tensorflow:Saving checkpoints for 0 into ./model_save/model.ckpt.\n",
      "INFO:tensorflow:Calling checkpoint listeners after saving checkpoint 0...\n",
      "INFO:tensorflow:loss = 674.3258, step = 0\n",
      "INFO:tensorflow:global_step/sec: 63.3819\n",
      "INFO:tensorflow:loss = 71.856544, step = 100 (1.579 sec)\n",
      "INFO:tensorflow:global_step/sec: 94.7996\n",
      "INFO:tensorflow:loss = 66.29317, step = 200 (1.055 sec)\n",
      "INFO:tensorflow:global_step/sec: 92.7462\n",
      "INFO:tensorflow:loss = 43.441273, step = 300 (1.079 sec)\n",
      "INFO:tensorflow:global_step/sec: 88.2287\n",
      "INFO:tensorflow:loss = 35.00142, step = 400 (1.133 sec)\n",
      "INFO:tensorflow:global_step/sec: 89.4839\n",
      "INFO:tensorflow:loss = 51.124264, step = 500 (1.118 sec)\n",
      "INFO:tensorflow:global_step/sec: 91.9273\n",
      "INFO:tensorflow:loss = 19.38278, step = 600 (1.088 sec)\n",
      "INFO:tensorflow:global_step/sec: 86.8627\n",
      "INFO:tensorflow:loss = 36.106926, step = 700 (1.153 sec)\n",
      "INFO:tensorflow:global_step/sec: 90.2678\n",
      "INFO:tensorflow:loss = 43.43695, step = 800 (1.106 sec)\n",
      "INFO:tensorflow:global_step/sec: 88.2671\n",
      "INFO:tensorflow:loss = 25.719534, step = 900 (1.133 sec)\n",
      "INFO:tensorflow:global_step/sec: 90.3493\n",
      "INFO:tensorflow:loss = 38.55125, step = 1000 (1.107 sec)\n",
      "INFO:tensorflow:global_step/sec: 91.7029\n",
      "INFO:tensorflow:loss = 40.877296, step = 1100 (1.090 sec)\n",
      "INFO:tensorflow:global_step/sec: 86.8288\n",
      "INFO:tensorflow:loss = 29.271692, step = 1200 (1.152 sec)\n",
      "INFO:tensorflow:global_step/sec: 91.8254\n",
      "INFO:tensorflow:loss = 32.639153, step = 1300 (1.089 sec)\n",
      "INFO:tensorflow:global_step/sec: 85.7017\n",
      "INFO:tensorflow:loss = 35.356888, step = 1400 (1.167 sec)\n",
      "INFO:tensorflow:global_step/sec: 91.5729\n",
      "INFO:tensorflow:loss = 18.747452, step = 1500 (1.092 sec)\n",
      "INFO:tensorflow:global_step/sec: 91.5779\n",
      "INFO:tensorflow:loss = 53.11138, step = 1600 (1.091 sec)\n",
      "INFO:tensorflow:global_step/sec: 91.5713\n",
      "INFO:tensorflow:loss = 28.432419, step = 1700 (1.092 sec)\n",
      "INFO:tensorflow:global_step/sec: 89.0071\n",
      "INFO:tensorflow:loss = 27.003002, step = 1800 (1.124 sec)\n",
      "INFO:tensorflow:global_step/sec: 89.6294\n",
      "INFO:tensorflow:loss = 36.353973, step = 1900 (1.115 sec)\n",
      "INFO:tensorflow:global_step/sec: 89.6752\n",
      "INFO:tensorflow:loss = 28.61785, step = 2000 (1.116 sec)\n",
      "INFO:tensorflow:global_step/sec: 88.1404\n",
      "INFO:tensorflow:loss = 35.618908, step = 2100 (1.134 sec)\n",
      "INFO:tensorflow:global_step/sec: 88.2978\n",
      "INFO:tensorflow:loss = 13.953371, step = 2200 (1.133 sec)\n",
      "INFO:tensorflow:global_step/sec: 92.0362\n",
      "INFO:tensorflow:loss = 14.43693, step = 2300 (1.086 sec)\n",
      "INFO:tensorflow:global_step/sec: 87.4543\n",
      "INFO:tensorflow:loss = 21.346413, step = 2400 (1.144 sec)\n",
      "INFO:tensorflow:global_step/sec: 88.7977\n",
      "INFO:tensorflow:loss = 16.169418, step = 2500 (1.125 sec)\n",
      "INFO:tensorflow:global_step/sec: 89.0521\n",
      "INFO:tensorflow:loss = 18.154087, step = 2600 (1.123 sec)\n",
      "INFO:tensorflow:global_step/sec: 88.7976\n",
      "INFO:tensorflow:loss = 22.469141, step = 2700 (1.126 sec)\n",
      "INFO:tensorflow:global_step/sec: 90.7023\n",
      "INFO:tensorflow:loss = 36.403248, step = 2800 (1.102 sec)\n",
      "INFO:tensorflow:global_step/sec: 89.0552\n",
      "INFO:tensorflow:loss = 30.166256, step = 2900 (1.123 sec)\n",
      "INFO:tensorflow:Calling checkpoint listeners before saving checkpoint 3000...\n",
      "INFO:tensorflow:Saving checkpoints for 3000 into ./model_save/model.ckpt.\n",
      "INFO:tensorflow:Calling checkpoint listeners after saving checkpoint 3000...\n",
      "INFO:tensorflow:Loss for final step: 15.064583.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow_estimator.python.estimator.canned.dnn.DNNRegressorV2 at 0x7f41bbdab750>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# InputFun函数，训练数据与验证数据所用的Input函数\n",
    "def InputFun(Features,Labels,Training,BatchSize):\n",
    "    Datasets=tf.data.Dataset.from_tensor_slices((dict(Features),Labels)) # 对数据加以加载\n",
    "    if Training:\n",
    "        Datasets=Datasets.shuffle(1000).repeat() # 对于训练数据，需要打乱（shuffle）、重复（repeat）\n",
    "    return Datasets.batch(BatchSize) # 将经过上述处理后的数据以每次BatchSize个输出\n",
    "\n",
    "# 基于训练数据训练模型\n",
    "DNNModel.train(input_fn=lambda:InputFun(TrainX,\n",
    "                                        TrainY,\n",
    "                                        True,\n",
    "                                        TrainBatchSize\n",
    "                                        ), # 调用InputFun函数；InputFun函数返回“tf.data.Dataset”对象，这个对象才可以被\n",
    "                                           # train函数识别并带入模型；由于InputFun函数每次返回BatchSize大小的数据个数，\n",
    "                                           # 因此需要多次执行，前面需要加lambda\n",
    "               steps=TrainStep # 指定模型训练的步数\n",
    "               )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2022-06-28T11:46:27\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from ./model_save/model.ckpt-3000\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Inference Time : 1.25740s\n",
      "INFO:tensorflow:Finished evaluation at 2022-06-28-11:46:28\n",
      "INFO:tensorflow:Saving dict for global step 3000: average_loss = 16.93486, global_step = 3000, label/mean = 24.235294, loss = 16.93486, prediction/mean = 24.88824\n",
      "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 3000: ./model_save/model.ckpt-3000\n",
      "ev:{'average_loss': 16.93486, 'label/mean': 24.235294, 'loss': 16.93486, 'prediction/mean': 24.88824, 'global_step': 3000}\n"
     ]
    }
   ],
   "source": [
    "# InputFunPredict函数，测试数据所用的Input函数\n",
    "def InputFunPredict(Features,BatchSize):\n",
    "    return tf.data.Dataset.from_tensor_slices(dict(Features)).batch(BatchSize) # 对数据加以加载,以每次BatchSize个输出    \n",
    "\n",
    "# 验证模型并保存验证结果\n",
    "EvalResult=DNNModel.evaluate(input_fn=lambda:InputFun(TestX,\n",
    "                                                      TestY,\n",
    "                                                      False,\n",
    "                                                      EvalBatchSize\n",
    "                                                      )\n",
    "                             )\n",
    "# 打印验证结果\n",
    "print('ev:{}'.format(EvalResult))\n",
    "\n",
    "# 基于测试数据测试模型精度结果\n",
    "PredictValues=DNNModel.predict(input_fn=lambda:InputFunPredict(TestX,\n",
    "                                                               PredictBatchSize\n",
    "                                                               )\n",
    "                               )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from ./model_save/model.ckpt-3000\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "24.447832\n",
      "25.685457\n",
      "23.968506\n",
      "24.93865\n",
      "24.560299\n",
      "25.95132\n",
      "27.829668\n",
      "21.868242\n",
      "24.69504\n",
      "24.807089\n",
      "22.39534\n",
      "25.75075\n",
      "26.379555\n",
      "26.218996\n",
      "25.424747\n",
      "23.91822\n",
      "24.260405\n",
      "[30, 26, 18, 25, 21, 19, 24, 25, 21, 21, 17, 26, 29, 30, 30, 29, 21] [24.447832, 25.685457, 23.968506, 24.93865, 24.560299, 25.95132, 27.829668, 21.868242, 24.69504, 24.807089, 22.39534, 25.75075, 26.379555, 26.218996, 25.424747, 23.91822, 24.260405]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "can only concatenate list (not \"int\") to list",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_14345/3514584379.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;31m# 调用AccuracyVerification函数，进行精度验证指标的计算与绘图\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m \u001b[0mAccuracyResult\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mAccuracyVerification\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPredictValues\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mTestY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0mPearsonR\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mR2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mRMSE\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mPredictY\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mAccuracyResult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mAccuracyResult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mAccuracyResult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mAccuracyResult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_14345/3514584379.py\u001b[0m in \u001b[0;36mAccuracyVerification\u001b[0;34m(PredictLabels, TestLabels)\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mTestLabels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTestLabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTestLabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPredictValuesList\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0mTestYList\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTestLabels\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0;31m# 以上为获取测试数据的因变量与模型预测所得的因变量\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mPearsonr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstats\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpearsonr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTestYList\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mPredictValuesList\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# 计算皮尔逊相关系数\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: can only concatenate list (not \"int\") to list"
     ]
    }
   ],
   "source": [
    "# AccuracyVerification函数，进行精度验证指标的计算与绘图\n",
    "def AccuracyVerification(PredictLabels,TestLabels):\n",
    "    value=0\n",
    "    PredictValuesList=[]\n",
    "    for k in PredictLabels:\n",
    "        value=k.get('predictions')[0]\n",
    "        print(value)\n",
    "        PredictValuesList.append(value)\n",
    "    TestLabels=TestLabels.values.tolist()\n",
    "    print(TestLabels, PredictValuesList)\n",
    "    TestYList=sum(TestLabels,[])\n",
    "    # 以上为获取测试数据的因变量与模型预测所得的因变量\n",
    "    Pearsonr=stats.pearsonr(TestYList,PredictValuesList) # 计算皮尔逊相关系数\n",
    "    R2=metrics.r2_score(TestYList,PredictValuesList) # 计算R方\n",
    "    RMSE=metrics.mean_squared_error(TestYList,PredictValuesList)**0.5 # 计算RMSE\n",
    "    plt.cla()\n",
    "    plt.plot(TestYList,PredictValuesList,'r*')\n",
    "    plt.xlabel('Actual Values')\n",
    "    plt.ylabel('Predicted Values')\n",
    "    # 以上为绘制拟合图像\n",
    "    print('Pearson correlation coefficient is {0}, and RMSE is {1}.'.format(Pearsonr[0],RMSE))\n",
    "    return (Pearsonr[0],R2,RMSE,PredictValuesList)\n",
    "\n",
    "# 调用AccuracyVerification函数，进行精度验证指标的计算与绘图\n",
    "AccuracyResult=AccuracyVerification(PredictValues,TestY)\n",
    "PearsonR,R2,RMSE,PredictY=AccuracyResult[0],AccuracyResult[1],AccuracyResult[2],AccuracyResult[3]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.13 ('SCD')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "cbdb981c21ad674d5d7cccac22161c0c4dbafc3564596111daa16bfc4bf9df64"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
