{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "from torch.utils.tensorboard import SummaryWriter   \n",
    "import pandas as pd,os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import numpy as np\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_predict, KFold\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from skorch import NeuralNetClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "# model_type = 'LSTM'\n",
    "# model_type = 'MLP'\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "# data_train, ans_train = np.load('./dataset4/X_train_class0_time.npy'), np.load('./dataset4/y_train_class0_time.npy', allow_pickle=True)\n",
    "# data_test, ans_test = np.load('./dataset4/X_test_class0_time.npy'), np.load('./dataset4/y_test_class0_time.npy', allow_pickle=True)\n",
    "\n",
    "# data_train, ans_train = np.load('./dataset4/X_train_class10_time.npy'), np.load('./dataset4/y_train_class10_time.npy', allow_pickle=True)\n",
    "# data_test, ans_test = np.load('./dataset4/X_test_class10_time.npy'), np.load('./dataset4/y_test_class10_time.npy', allow_pickle=True)\n",
    "\n",
    "# data_train, ans_train = np.load('./dataset4/X_train_class50_time.npy'), np.load('./dataset4/y_train_class50_time.npy', allow_pickle=True)\n",
    "# data_test, ans_test = np.load('./dataset4/X_test_class50_time.npy'), np.load('./dataset4/y_test_class50_time.npy', allow_pickle=True)\n",
    "\n",
    "data_train, ans_train = np.load('./dataset4/X_train_class100_time.npy'), np.load('./dataset4/y_train_class100_time.npy', allow_pickle=True)\n",
    "data_test, ans_test = np.load('./dataset4/X_test_class100_time.npy'), np.load('./dataset4/y_test_class100_time.npy', allow_pickle=True)\n",
    "\n",
    "\n",
    "\n",
    "label_train = np.unique(ans_train, return_counts=False,return_index=False,return_inverse=False)\n",
    "label_test = np.unique(ans_test, return_counts=False,return_index=False,return_inverse=False)\n",
    "print(len(label_train), len(label_test))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "7 7\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "def label2value(ans, data):\n",
    "    X = []\n",
    "    y = []\n",
    "    \n",
    "    for idi,i in enumerate(ans):\n",
    "        pop = False\n",
    "        for idj, j in enumerate(label_train):\n",
    "            if j==i:\n",
    "                pop = True\n",
    "                y.append(idj)\n",
    "        if pop==True:\n",
    "            X.append(data[idi])\n",
    "    return np.array(X), np.array(y)\n",
    "X_train, y_train = label2value(ans_train, data_train)\n",
    "X_test, y_test = label2value(ans_test, data_test)\n",
    "print(len(np.unique(y_train, return_counts=False,return_index=False,return_inverse=False)),len(np.unique(y_test, return_counts=False,return_index=False,return_inverse=False)))\n",
    "print(X_train.shape, y_train.shape, X_test.shape, y_test.shape)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "7 7\n",
      "(1696, 177848) (1696,) (566, 177848) (566,)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self, class_n, Input_Size):\n",
    "        super(RNN, self).__init__()\n",
    "\n",
    "        self.rnn = nn.LSTM(\n",
    "            input_size=Input_Size,\n",
    "            hidden_size=128,\n",
    "            num_layers=1,\n",
    "            batch_first=True,\n",
    "        )\n",
    "\n",
    "        self.out = nn.Linear(128, class_n)\n",
    "    def forward(self, x):\n",
    "        r_out, (h_c, h_h) = self.rnn(x, None)\n",
    "        out = self.out(r_out[:, -1, :])\n",
    "        return out\n",
    "    \n",
    "    def initialize_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                torch.nn.init.normal_(m.weight.data, 0, 0.01)\n",
    "                # m.weight.data.normal_(0,0.01)\n",
    "                m.bias.data.zero_()\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, num_classes, input_size):\n",
    "        super(MLP,self).__init__()\n",
    "        self.linear1 = nn.Linear(in_features=input_size, out_features=1024)\n",
    "        self.bn1 = nn.BatchNorm1d(1024)\n",
    "        self.dt1 = nn.Dropout(0.25)\n",
    "        self.linear2 = nn.Linear(in_features=1024, out_features=256)\n",
    "        self.bn2 = nn.BatchNorm1d(256)\n",
    "        self.dt2 = nn.Dropout(0.25)\n",
    "        self.linear3 = nn.Linear(in_features=256, out_features= num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.bn1(self.linear1(x))\n",
    "        x = F.relu(x)\n",
    "        # x = self.dt1(x)\n",
    "        x = self.bn2(self.linear2(x))\n",
    "        x = F.relu(x)\n",
    "        # x = self.dt2(x)\n",
    "        x = self.linear3(x)\n",
    "        x = F.softmax(x, dim=1)\n",
    "        return x\n",
    "        \n",
    "    def initialize_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.BatchNorm1d):\n",
    "                m.weight.data.fill_(1)\n",
    "                m.bias.data.zero_()\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                torch.nn.init.normal_(m.weight.data, 0, 0.01)\n",
    "                # m.weight.data.normal_(0,0.01)\n",
    "                m.bias.data.zero_()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "LR = 0.0001\n",
    "epochs = 380\n",
    "batch_size = 128\n",
    "k_cv = KFold(n_splits=10, random_state=42, shuffle=True) \n",
    "\n",
    "device = ('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "if hasattr(torch.cuda, 'empty_cache'):\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "def accuracy(predictions, labels):\n",
    "    classes = torch.argmax(predictions, dim=1)\n",
    "    return torch.mean((classes == labels).float())\n",
    "\n",
    "def trainer(idx, train_index, valid_index):\n",
    "    \n",
    "    X_, x_ = X_train[train_index], X_train[valid_index]\n",
    "    Y_, y_ = y_train[train_index], y_train[valid_index]\n",
    "    print(X_.shape, x_.shape)\n",
    "    train_zip = TensorDataset(torch.tensor(X_), torch.tensor(Y_))\n",
    "    valid_zip = TensorDataset(torch.tensor(x_), torch.tensor(y_))\n",
    "    train_loader = DataLoader(dataset=train_zip, batch_size=batch_size, shuffle=True)\n",
    "    valid_loader = DataLoader(dataset=valid_zip, batch_size=batch_size, shuffle=False)\n",
    "    train_Ls = []\n",
    "    valid_Ls = []\n",
    "    # Starting trainer\n",
    "    Input_Size = np.array(X_train).shape[1]\n",
    "    if model_type=='LSTM':\n",
    "        model = RNN(max(y_train)+1, Input_Size).to(device)\n",
    "    else:\n",
    "        model = MLP(max(y_train)+1, Input_Size).to(device)\n",
    "    # model.initialize_weights()\n",
    "    loss_fun = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=LR)\n",
    "    min_valid_loss = np.inf\n",
    "    for epoch in tqdm(range(epochs)):\n",
    "            correct = 0\n",
    "            for step, (b_x, b_y) in enumerate(train_loader):\n",
    "                # Forward pass\n",
    "                model.train()\n",
    "                train_loss = 0.0\n",
    "                if model_type=='LSTM':\n",
    "                    b_x_, b_y_= (torch.unsqueeze(b_x, 1)).data.type(torch.FloatTensor).to(device), b_y.to(device)\n",
    "                else:\n",
    "                    b_x_, b_y_= b_x.float().to(device), b_y.to(device)\n",
    "                optimizer.zero_grad()\n",
    "                r_out = model(b_x_)\n",
    "                loss = loss_fun(r_out, b_y_)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                train_loss = loss.item()*b_x_.size(0)\n",
    "                correct = correct + accuracy(r_out, b_y_)\n",
    "                del b_x_, b_y_\n",
    "                torch.cuda.empty_cache()\n",
    "            train_acc = 100 * correct / len(train_loader)\n",
    "            train_writer.add_scalar('accuracy', train_acc, epoch)\n",
    "            train_writer.add_scalar('losses', train_loss, epoch)\n",
    "            train_Ls.append(train_loss)\n",
    "            # Validation\n",
    "            correct2=0\n",
    "            with torch.no_grad():\n",
    "                valid_loss = 0.0\n",
    "                model.eval()\n",
    "                for step, (b_x, b_y) in enumerate(valid_loader):\n",
    "                    if model_type=='LSTM':\n",
    "                        b_x_, b_y_= (torch.unsqueeze(b_x, 1)).data.type(torch.FloatTensor).to(device), b_y.to(device)\n",
    "                    else:\n",
    "                        b_x_, b_y_= b_x.float().to(device), b_y.to(device)\n",
    "                    r_out = model(b_x_)\n",
    "                    loss = loss_fun(r_out, b_y_)\n",
    "                    valid_loss = loss.item()*b_x_.size(0)\n",
    "                    v_pred = torch.argmax(r_out, dim=1)\n",
    "                    # print(accuracy(r_out, b_y_))\n",
    "                    correct2  = correct2 + accuracy(r_out, b_y_)\n",
    "                    del b_x_, b_y_\n",
    "                    torch.cuda.empty_cache()\n",
    "\n",
    "                if min_valid_loss > valid_loss:\n",
    "                    min_valid_loss = valid_loss\n",
    "                    torch.save({'state_dict': model.state_dict()}, f'./pth/{model_type + \"_\"+str(idx+1)}_saved_model_class{max(y_train)+1}.pth.tar')\n",
    "\n",
    "            valid_acc = 100 * correct2 / len(valid_loader)\n",
    "            valid_writer.add_scalar('accuracy', valid_acc, epoch)\n",
    "            valid_writer.add_scalar('losses', valid_loss, epoch)\n",
    "            valid_Ls.append(valid_loss)     \n",
    "    return [train_Ls, valid_Ls]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "import matplotlib.pyplot as plt\n",
    "md_type_stack= ['MLP', 'LSTM']\n",
    "for j in md_type_stack:\n",
    "    model_type = j\n",
    "    for idx,(train_index, valid_index) in enumerate(k_cv.split(X_train)):\n",
    "        log_dir =os.path.join('./path/to/log/',f'{model_type}_{max(y_train)+1}_{idx+1}')\n",
    "        train_writer = SummaryWriter(log_dir=log_dir+'/train')\n",
    "        valid_writer = SummaryWriter(log_dir=log_dir+'/valid')\n",
    "        loss_stack = trainer(idx, train_index, valid_index)\n",
    "        \n",
    "        fig = plt.figure(figsize=(15,5))\n",
    "        plt.plot(loss_stack[0], label='train')\n",
    "        plt.plot(loss_stack[1], label='test')\n",
    "        plt.title(f'Kfold - {idx+1} {model_type} Class {max(y_train)+1}')\n",
    "        plt.legend()\n",
    "        if not os.path.exists(f'Results/'):\n",
    "            os.makedirs(f'Results/')\n",
    "        plt.savefig(f'Results/Kfold - {idx+1} {model_type} Class {max(y_train)+1} Training Plot.jpg')\n",
    "        plt.cla()\n",
    "        plt.clf()\n",
    "        plt.close()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(1526, 177848) (170, 177848)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 380/380 [11:32<00:00,  1.82s/it]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(1526, 177848) (170, 177848)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 380/380 [11:40<00:00,  1.84s/it]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(1526, 177848) (170, 177848)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 380/380 [11:08<00:00,  1.76s/it]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(1526, 177848) (170, 177848)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 380/380 [11:52<00:00,  1.88s/it]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(1526, 177848) (170, 177848)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 380/380 [12:05<00:00,  1.91s/it]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(1526, 177848) (170, 177848)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 380/380 [11:42<00:00,  1.85s/it]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(1527, 177848) (169, 177848)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 380/380 [10:27<00:00,  1.65s/it]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(1527, 177848) (169, 177848)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 380/380 [11:35<00:00,  1.83s/it]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(1527, 177848) (169, 177848)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 380/380 [11:18<00:00,  1.79s/it]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(1527, 177848) (169, 177848)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 380/380 [10:30<00:00,  1.66s/it]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(1526, 177848) (170, 177848)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 380/380 [10:21<00:00,  1.64s/it]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(1526, 177848) (170, 177848)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 380/380 [09:57<00:00,  1.57s/it]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(1526, 177848) (170, 177848)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 380/380 [11:16<00:00,  1.78s/it]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(1526, 177848) (170, 177848)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 380/380 [10:26<00:00,  1.65s/it]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(1526, 177848) (170, 177848)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 380/380 [09:43<00:00,  1.54s/it]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(1526, 177848) (170, 177848)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 380/380 [09:02<00:00,  1.43s/it]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(1527, 177848) (169, 177848)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 380/380 [08:11<00:00,  1.29s/it]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(1527, 177848) (169, 177848)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 380/380 [09:48<00:00,  1.55s/it]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(1527, 177848) (169, 177848)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 380/380 [10:59<00:00,  1.74s/it]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(1527, 177848) (169, 177848)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 380/380 [09:05<00:00,  1.44s/it]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "if model_type=='LSTM':\n",
    "    net = NeuralNetClassifier(module=RNN, train_split=None)\n",
    "else:\n",
    "    net = NeuralNetClassifier(module=MLP, train_split=None)\n",
    "\n",
    "\n",
    "# net = NeuralNetClassifier(module=RNN, train_split=None)\n",
    "# y_pred = cross_val_predict(net, X_train, y_train, cv=k_cv)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "# # model loadings\n",
    "# if model_type=='LSTM':\n",
    "#         model = RNN(max(y_train)+1, Input_Size).to(device)\n",
    "# else:\n",
    "#         model = MLP(max(y_train)+1, Input_Size).to(device)\n",
    "# # checkpoint = torch.load(f'./pth/{model_type}_saved_model_50_epoch-1000.pth.tar', map_location='cpu')\n",
    "# checkpoint = torch.load(f'./pth/{model_type}_saved_model_50_epoch-1000.pth.tar')\n",
    "# model.load_state_dict(checkpoint['state_dict'])\n",
    "# print(model)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "# DNA_list = ['-', 'A', 'C', 'G', 'N', 'T']\n",
    "# print(X_train.shape[1])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "# fn = pd.read_csv('../lstm covid19/feature_name.csv')\n",
    "\n",
    "# fn = np.array(fn.values)\n",
    "# fn = fn.flatten()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "# # def index2text(value_data):\n",
    "# #     text=[]\n",
    "# #     for id1, n1 in enumerate(np.array(value_data)):\n",
    "# #         for id2, n2 in enumerate(feature_text):\n",
    "# #             if n1==id2:\n",
    "# #                 add_ = str(n2)+'_'+str(id1)\n",
    "# #                 text.append(add_)\n",
    "# #     return text\n",
    "# import matplotlib.pyplot as plt\n",
    "# import shap\n",
    "# shap.initjs()\n",
    "# # class_n = 3 #0-7\n",
    "# # for f_id, class_n in enumerate(label_train):\n",
    "# f_id = 0\n",
    "# class_n = label_test[f_id]\n",
    "# batch = next(iter(valid_loader))\n",
    "# value, ans = batch\n",
    "# new_value = []\n",
    "# # for id, i in enumerate(ans):\n",
    "#     # if i==f_id:\n",
    "#         # new_value.append(np.array(value[id]))\n",
    "\n",
    "# # background = torch.tensor(new_value)\n",
    "# background = value\n",
    "# if model_type=='LSTM':\n",
    "#     background_ex = (torch.unsqueeze(background, 1)).data.type(torch.FloatTensor)\n",
    "# else:\n",
    "#     background_ex = background.data.type(torch.FloatTensor)\n",
    "\n",
    "# print('Test Data Shape ',value.shape)\n",
    "# print('Extract Data Shape ', background.shape)\n",
    "# print('Squeeze Data Shape ', background_ex.shape)\n",
    "\n",
    "# with torch.no_grad():\n",
    "#     e = shap.DeepExplainer(model.cpu(), background_ex.cpu())\n",
    "# shap_values = e.shap_values(background_ex.cpu())\n",
    "\n",
    "# test_stack = []\n",
    "# # for i in range(len(new_value)):\n",
    "# #     temp = index2text(new_value[i])\n",
    "# #     test_stack.append(temp)\n",
    "# test_stack = np.array(value)\n",
    "# # test_stack = np.array(test_stack)\n",
    "# print('Ground Turth Data Shape ', test_stack.shape)\n",
    "# if model_type=='LSTM':\n",
    "#     shap_values_ = np.squeeze(np.array(shap_values),axis=-2)\n",
    "# else:\n",
    "#     shap_values_ = np.array(shap_values)\n",
    "\n",
    "# print('Shap Values Shape ', shap_values_.shape)\n",
    "\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "# f_id = 6\n",
    "# class_n = label_test[f_id]\n",
    "# print(shap_values_[f_id].shape, test_stack.shape)\n",
    "# # for f_id, class_n in enumerate(label_train):\n",
    "# shap.summary_plot(list(shap_values_), test_stack, plot_type=\"bar\", class_names=label_test, feature_names=fn, show=False)\n",
    "# fig = plt.gcf()\n",
    "# fig.set_figheight(12)\n",
    "# fig.set_figwidth(14)\n",
    "# ax = plt.gca()\n",
    "# ax.set_ylabel('Feature', fontsize=16)\n",
    "# leg = ax.legend()\n",
    "\n",
    "# plt.title(f'{model_type} Multiclass', fontsize=20)\n",
    "# # plt.show()\n",
    "# plt.rcParams[\"figure.figsize\"] = (10,20)\n",
    "# plt.savefig(f'/ssd1/報告//Images//DNA/{model_type}//{model_type} multiclass-100-output.jpg', dpi=300)\n",
    "# # shap.summary_plot(shap_values, X)"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.6.13",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.6.13 64-bit ('lstm_pyt': conda)"
  },
  "interpreter": {
   "hash": "47ed0dbdec9901086886300defac8fc027ffa2cf340ed530b82da576181f4925"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}