{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter   \n",
    "import pandas as pd,os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import numpy as np\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_predict, KFold\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self, class_n, Input_Size):\n",
    "        super(RNN, self).__init__()\n",
    "\n",
    "        self.rnn = nn.LSTM(\n",
    "            input_size=Input_Size,\n",
    "            hidden_size=128,\n",
    "            num_layers=1,\n",
    "            batch_first=True,\n",
    "        )\n",
    "\n",
    "        self.out = nn.Linear(128, class_n)\n",
    "    def forward(self, x):\n",
    "        r_out, (h_c, h_h) = self.rnn(x, None)\n",
    "        out = self.out(r_out[:, -1, :])\n",
    "        return out\n",
    "    \n",
    "    def initialize_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                torch.nn.init.normal_(m.weight.data, 0, 0.01)\n",
    "                # m.weight.data.normal_(0,0.01)\n",
    "                m.bias.data.zero_()\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, num_classes, input_size):\n",
    "        super(MLP,self).__init__()\n",
    "        self.linear1 = nn.Linear(in_features=input_size, out_features=1024)\n",
    "        self.bn1 = nn.BatchNorm1d(1024)\n",
    "        self.dt1 = nn.Dropout(0.25)\n",
    "        self.linear2 = nn.Linear(in_features=1024, out_features=256)\n",
    "        self.bn2 = nn.BatchNorm1d(256)\n",
    "        self.dt2 = nn.Dropout(0.25)\n",
    "        self.linear3 = nn.Linear(in_features=256, out_features= num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.bn1(self.linear1(x))\n",
    "        x = F.relu(x)\n",
    "        # x = self.dt1(x)\n",
    "        x = self.bn2(self.linear2(x))\n",
    "        x = F.relu(x)\n",
    "        # x = self.dt2(x)\n",
    "        x = self.linear3(x)\n",
    "        x = F.softmax(x, dim=1)\n",
    "        return x\n",
    "        \n",
    "    def initialize_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.BatchNorm1d):\n",
    "                m.weight.data.fill_(1)\n",
    "                m.bias.data.zero_()\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                torch.nn.init.normal_(m.weight.data, 0, 0.01)\n",
    "                # m.weight.data.normal_(0,0.01)\n",
    "                m.bias.data.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label2value(ans, data, label_train):\n",
    "    X = []\n",
    "    y = []\n",
    "    \n",
    "    for idi,i in enumerate(ans):\n",
    "        pop = False\n",
    "        for idj, j in enumerate(label_train):\n",
    "            if j==i:\n",
    "                pop = True\n",
    "                y.append(idj)\n",
    "        if pop==True:\n",
    "            X.append(data[idi])\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "def dataloader(subg_n):\n",
    "    subgroup_ = {'class55':10, 'class7':100, 'class8': 50}\n",
    "    data_train, ans_train= np.load(f'./dataset4/X_train_class{subgroup_[subg_n]}_time.npy'), np.load(f'./dataset4/y_train_class{subgroup_[subg_n]}_time.npy', allow_pickle=True)\n",
    "    label_train = np.unique(ans_train, return_counts=False,return_index=False,return_inverse=False)\n",
    "\n",
    "    X_train, y_train = label2value(ans_train, data_train, label_train)\n",
    "    # print(len(np.unique(y_train, return_counts=False,return_index=False,return_inverse=False)))\n",
    "    # print(X_train.shape, y_train.shape)\n",
    "    return X_train, y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self, class_n, Input_Size):\n",
    "        super(RNN, self).__init__()\n",
    "\n",
    "        self.rnn = nn.LSTM(\n",
    "            input_size=Input_Size,\n",
    "            hidden_size=128,\n",
    "            num_layers=1,\n",
    "            batch_first=True,\n",
    "        )\n",
    "\n",
    "        self.out = nn.Linear(128, class_n)\n",
    "    def forward(self, x):\n",
    "        r_out, (h_c, h_h) = self.rnn(x, None)\n",
    "        out = self.out(r_out[:, -1, :])\n",
    "        return out\n",
    "    \n",
    "    def initialize_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                torch.nn.init.normal_(m.weight.data, 0, 0.01)\n",
    "                # m.weight.data.normal_(0,0.01)\n",
    "                m.bias.data.zero_()\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, num_classes, input_size):\n",
    "        super(MLP,self).__init__()\n",
    "        self.linear1 = nn.Linear(in_features=input_size, out_features=1024)\n",
    "        self.bn1 = nn.BatchNorm1d(1024)\n",
    "        self.dt1 = nn.Dropout(0.25)\n",
    "        self.linear2 = nn.Linear(in_features=1024, out_features=256)\n",
    "        self.bn2 = nn.BatchNorm1d(256)\n",
    "        self.dt2 = nn.Dropout(0.25)\n",
    "        self.linear3 = nn.Linear(in_features=256, out_features= num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.bn1(self.linear1(x))\n",
    "        x = F.relu(x)\n",
    "        # x = self.dt1(x)\n",
    "        x = self.bn2(self.linear2(x))\n",
    "        x = F.relu(x)\n",
    "        # x = self.dt2(x)\n",
    "        x = self.linear3(x)\n",
    "        x = F.softmax(x, dim=1)\n",
    "        return x\n",
    "        \n",
    "    def initialize_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.BatchNorm1d):\n",
    "                m.weight.data.fill_(1)\n",
    "                m.bias.data.zero_()\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                torch.nn.init.normal_(m.weight.data, 0, 0.01)\n",
    "                # m.weight.data.normal_(0,0.01)\n",
    "                m.bias.data.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model:  MLP class:  class55\n",
      "Best Accuracy 96.34878540039062 %\n",
      "model:  MLP class:  class7\n",
      "Best Accuracy 98.41889953613281 %\n",
      "model:  MLP class:  class8\n",
      "Best Accuracy 98.95833587646484 %\n",
      "model:  LSTM class:  class55\n",
      "Best Accuracy 73.99462127685547 %\n",
      "model:  LSTM class:  class7\n",
      "Best Accuracy 98.046875 %\n",
      "model:  LSTM class:  class8\n",
      "Best Accuracy 98.17708587646484 %\n"
     ]
    }
   ],
   "source": [
    "batch_size = 128\n",
    "k_cv = KFold(n_splits=10, random_state=42, shuffle=True) \n",
    "\n",
    "def accuracy(predictions, labels):\n",
    "    classes = torch.argmax(predictions, dim=1)\n",
    "    return torch.mean((classes == labels).float())\n",
    "\n",
    "def predict(idx, valid_index, model_weight):\n",
    "    x_, y_ = X_train[valid_index], y_train[valid_index]\n",
    "    valid_zip = TensorDataset(torch.tensor(x_), torch.tensor(y_))\n",
    "    valid_loader = DataLoader(dataset=valid_zip, batch_size=batch_size, shuffle=False)\n",
    "    # Starting trainer\n",
    "    Input_Size = np.array(X_train).shape[1]\n",
    "    checkpoint = torch.load('./pth/'+model_weight, map_location=torch.device('cpu'))\n",
    "    if 'LSTM' in model_weight:\n",
    "        model = RNN(max(y_train)+1, Input_Size).to('cpu')\n",
    "    else:\n",
    "        model = MLP(max(y_train)+1, Input_Size).to('cpu')\n",
    "    model.load_state_dict(checkpoint['state_dict'])\n",
    "    model.to('cpu')\n",
    "    # Validation\n",
    "    correct2=0\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        for step, (b_x, b_y) in enumerate(valid_loader):\n",
    "            if 'LSTM' in model_weight:\n",
    "                b_x_, b_y_= (torch.unsqueeze(b_x, 1)).data.type(torch.FloatTensor).to('cpu'), b_y.to('cpu')\n",
    "            else:\n",
    "                b_x_, b_y_= b_x.float().to('cpu'), b_y.to('cpu')\n",
    "            r_out = model(b_x_)\n",
    "            correct2  = correct2 + accuracy(r_out, b_y_)\n",
    "\n",
    "            torch.cuda.empty_cache()\n",
    "    valid_acc = 100 * correct2 / len(valid_loader)\n",
    "    return valid_acc.item()\n",
    "\n",
    "md_type_stack= ['MLP', 'LSTM']\n",
    "# md_type_stack= ['MLP']\n",
    "subgroup_dataset = ['class55', 'class7', 'class8']\n",
    "# subgroup_dataset = ['class55']\n",
    "weight_lsit = sorted(os.listdir('./pth'))\n",
    "for j in md_type_stack:\n",
    "    for jj in subgroup_dataset:\n",
    "        acc = []\n",
    "        X_train, y_train = dataloader(jj)\n",
    "        next_weight = [i for i in weight_lsit if j in i and jj in i]\n",
    "        temp  = next_weight[1::]\n",
    "        temp.append(next_weight[0])\n",
    "        next_weight = temp\n",
    "        for idx,(train_index, valid_index) in enumerate(k_cv.split(X_train)):\n",
    "            acc.append(predict(idx, valid_index, next_weight[idx]))\n",
    "        print('model: ',j ,'class: ' ,jj)\n",
    "        print('Best Accuracy', max(acc), '%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "cefeaee0cb99e52f47ecbf6a0fec4d636206690d7e9c62031f057a9471691d65"
  },
  "kernelspec": {
   "display_name": "Python 3.6.13 64-bit ('lstm_pyt': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
